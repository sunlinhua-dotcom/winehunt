"""
Wine-Searcher çˆ¬è™«æ¨¡å— â€” åŒå¼•æ“ååçˆ¬ç‰ˆ
ä¸»å¼•æ“: curl_cffi (æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ TLS/JA3 æŒ‡çº¹)
å¤‡å¼•æ“: ScraperAPI å…è´¹å±‚ (5000 æ¬¡/æœˆ)
"""
import asyncio
import random
import re
import os
import json
import logging
from typing import Optional
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

# â”€â”€ é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_URL = "https://www.wine-searcher.com"

# ScraperAPI å…è´¹ keyï¼ˆç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
SCRAPER_API_KEY = os.getenv("SCRAPER_API_KEY", "")

# User-Agent æ± 
USER_AGENTS = [
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.2 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:133.0) Gecko/20100101 Firefox/133.0",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
]

# TLS æŒ‡çº¹æ¨¡æ‹Ÿçš„æµè§ˆå™¨ç§ç±»ï¼ˆä»…ä½¿ç”¨ curl_cffi ç¡®è®¤æ”¯æŒçš„ï¼‰
IMPERSONATES = [
    "chrome124",
    "chrome120",
    "chrome119",
    "chrome116",
    "chrome110",
    "chrome107",
    "chrome104",
    "chrome101",
    "chrome100",
    "chrome99",
    "safari15_5",
    "safari15_3",
]


# â”€â”€ å¼•æ“ 1: curl_cffiï¼ˆä¸»å¼•æ“ï¼Œå¸¦ session é¢„çƒ­ï¼‰â”€â”€â”€â”€â”€â”€
async def _fetch_with_curl_cffi(url: str, max_retries: int = 3) -> Optional[str]:
    """
    ä½¿ç”¨ curl_cffi æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ TLS æŒ‡çº¹
    ç­–ç•¥: å…ˆè®¿é—®ä¸»é¡µè·å– Cloudflare cookie â†’ å†ç”¨åŒä¸€ session æœç´¢
    """
    try:
        from curl_cffi.requests import AsyncSession
    except ImportError:
        logger.warning("curl_cffi æœªå®‰è£…ï¼Œè·³è¿‡æ­¤å¼•æ“")
        return None

    for attempt in range(max_retries):
        try:
            impersonate = random.choice(IMPERSONATES)
            ua = random.choice(USER_AGENTS)
            headers = {
                "User-Agent": ua,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.9",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Ch-Ua": '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
                "Sec-Ch-Ua-Mobile": "?0",
                "Sec-Ch-Ua-Platform": '"macOS"',
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
                "Cache-Control": "max-age=0",
            }

            async with AsyncSession(impersonate=impersonate) as session:
                # Step 1: é¢„çƒ­ â€” å…ˆè®¿é—®ä¸»é¡µæ‹¿ cookieï¼ˆæ¨¡æ‹ŸçœŸäººå…ˆæ‰“å¼€ç½‘ç«™é¦–é¡µï¼‰
                warmup_headers = dict(headers)
                try:
                    warmup = await session.get(
                        BASE_URL,
                        headers=warmup_headers,
                        timeout=20,
                        allow_redirects=True,
                    )
                    logger.debug(f"é¢„çƒ­çŠ¶æ€: {warmup.status_code} ({impersonate})")
                    await asyncio.sleep(random.uniform(1.5, 4))
                except Exception as e:
                    logger.debug(f"é¢„çƒ­å¤±è´¥ (ç»§ç»­å°è¯•): {e}")

                # Step 2: çœŸæ­£çš„æœç´¢è¯·æ±‚ï¼Œå¸¦ä¸Š Referer æ¨¡æ‹Ÿç«™å†…æµè§ˆ
                search_headers = dict(headers)
                search_headers["Referer"] = BASE_URL + "/"
                search_headers["Sec-Fetch-Site"] = "same-origin"

                resp = await session.get(
                    url,
                    headers=search_headers,
                    timeout=30,
                    allow_redirects=True,
                )

                if resp.status_code == 200:
                    logger.info(f"âœ… curl_cffi æˆåŠŸ ({impersonate}): {url[:80]}")
                    return resp.text
                elif resp.status_code == 403:
                    logger.warning(f"curl_cffi 403 (å°è¯• {attempt+1}/{max_retries}, {impersonate}): {url[:80]}")
                    await asyncio.sleep(random.uniform(5, 12))
                    continue
                else:
                    logger.warning(f"curl_cffi {resp.status_code}: {url[:80]}")
                    return None

        except Exception as e:
            logger.warning(f"curl_cffi å¼‚å¸¸ (å°è¯• {attempt+1}): {e}")
            await asyncio.sleep(random.uniform(3, 6))
            continue

    return None


# â”€â”€ å¼•æ“ 1: ScraperAPIï¼ˆä¸»å¼•æ“ï¼Œä»£ç† IP ç»•è¿‡å°é”ï¼‰â”€â”€â”€â”€
async def _fetch_with_scraper_api(url: str) -> Optional[str]:
    """
    ä½¿ç”¨ ScraperAPI å…è´¹å±‚ï¼ˆ5000 æ¬¡/æœˆï¼‰
    é€šè¿‡ä»£ç† IP ç»•è¿‡ Wine-Searcher çš„ IP å°æ€
    """
    if not SCRAPER_API_KEY:
        logger.debug("æœªé…ç½® SCRAPER_API_KEYï¼Œè·³è¿‡ ScraperAPI")
        return None

    import httpx
    api_url = "https://api.scraperapi.com"
    params = {
        "api_key": SCRAPER_API_KEY,
        "url": url,
        "render": "false",
    }

    for attempt in range(2):  # æœ€å¤šé‡è¯• 2 æ¬¡
        try:
            async with httpx.AsyncClient(timeout=90) as client:
                resp = await client.get(api_url, params=params)
                if resp.status_code == 200:
                    logger.info(f"âœ… ScraperAPI æˆåŠŸ: {url[:80]}")
                    return resp.text
                elif resp.status_code in (500, 502, 503):
                    logger.warning(f"ScraperAPI {resp.status_code} é‡è¯• ({attempt+1}/2): {url[:80]}")
                    await asyncio.sleep(random.uniform(3, 6))
                    continue
                else:
                    logger.warning(f"ScraperAPI {resp.status_code}: {url[:80]}")
                    return None
        except Exception as e:
            logger.warning(f"ScraperAPI å¼‚å¸¸ ({attempt+1}/2): {e}")
            await asyncio.sleep(random.uniform(2, 4))
            continue

    return None


# â”€â”€ å¼•æ“ 3: httpx åŸºç¡€è¯·æ±‚ï¼ˆæœ€åå¤‡ç”¨ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def _fetch_with_httpx(url: str) -> Optional[str]:
    """æœ€åŸºç¡€çš„ httpx è¯·æ±‚"""
    try:
        import httpx
        headers = {
            "User-Agent": random.choice(USER_AGENTS),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
        }
        async with httpx.AsyncClient(follow_redirects=True, timeout=30) as client:
            resp = await client.get(url, headers=headers)
            if resp.status_code == 200:
                logger.info(f"âœ… httpx æˆåŠŸ: {url[:80]}")
                return resp.text
            logger.warning(f"httpx {resp.status_code}: {url[:80]}")
            return None
    except Exception as e:
        logger.warning(f"httpx å¼‚å¸¸: {e}")
        return None


# â”€â”€ ç»Ÿä¸€è¯·æ±‚å‡½æ•°ï¼ˆç€‘å¸ƒå¼é™çº§ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def _smart_fetch(url: str) -> Optional[str]:
    """
    ä¾æ¬¡å°è¯•ä¸‰ä¸ªå¼•æ“ï¼Œç›´åˆ°æˆåŠŸï¼š
    1. ScraperAPIï¼ˆå…è´¹ä»£ç†ï¼Œç»•è¿‡ IP å°é”ï¼Œæœ€å¯é ï¼‰
    2. curl_cffiï¼ˆTLS æŒ‡çº¹ä¼ªè£…ï¼ŒIP æœªå°æ—¶å¯ç”¨ï¼‰
    3. httpxï¼ˆåŸºç¡€è¯·æ±‚ï¼Œå…œåº•ï¼‰
    """
    # å¼•æ“ 1: ScraperAPIï¼ˆä¼˜å…ˆï¼Œç»•è¿‡ IP å°é”ï¼‰
    html = await _fetch_with_scraper_api(url)
    if html:
        return html

    # å¼•æ“ 2: curl_cffiï¼ˆIP æœªå°æ—¶æœ‰æ•ˆï¼‰
    html = await _fetch_with_curl_cffi(url, max_retries=1)
    if html:
        return html

    # å¼•æ“ 3: httpxï¼ˆå…œåº•ï¼‰
    html = await _fetch_with_httpx(url)
    if html:
        return html

    logger.error(f"âŒ æ‰€æœ‰å¼•æ“å‡å¤±è´¥: {url[:80]}")
    return None


# â”€â”€ éšæœºå»¶è¿Ÿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def _random_delay(min_sec: float = 3, max_sec: float = 8):
    """æ¨¡æ‹Ÿäººç±»æµè§ˆé—´éš”"""
    await asyncio.sleep(random.uniform(min_sec, max_sec))


# â”€â”€ ä»·æ ¼è§£æ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _parse_price(price_text: str) -> Optional[float]:
    if not price_text:
        return None
    cleaned = re.sub(r'[^\d.,]', '', price_text.strip())
    if ',' in cleaned and '.' in cleaned:
        if cleaned.index(',') > cleaned.index('.'):
            cleaned = cleaned.replace('.', '').replace(',', '.')
        else:
            cleaned = cleaned.replace(',', '')
    elif ',' in cleaned:
        parts = cleaned.split(',')
        if len(parts[-1]) == 2:
            cleaned = cleaned.replace(',', '.')
        else:
            cleaned = cleaned.replace(',', '')
    try:
        return float(cleaned)
    except ValueError:
        return None


def _detect_currency(price_text: str) -> str:
    if '$' in price_text or 'USD' in price_text:
        return 'USD'
    elif 'â‚¬' in price_text or 'EUR' in price_text:
        return 'EUR'
    elif 'Â£' in price_text or 'GBP' in price_text:
        return 'GBP'
    elif 'HK$' in price_text or 'HKD' in price_text:
        return 'HKD'
    elif 'Â¥' in price_text or 'CNY' in price_text:
        return 'CNY'
    return 'USD'


EXCHANGE_RATES = {
    'USD': 1.0, 'EUR': 1.08, 'GBP': 1.27,
    'HKD': 0.128, 'CNY': 0.14, 'AUD': 0.65,
    'JPY': 0.0067, 'CHF': 1.12,
}


def _to_usd(price: float, currency: str) -> float:
    return price * EXCHANGE_RATES.get(currency, 1.0)


# â”€â”€ é¡µé¢è§£æ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _parse_wine_page(html: str) -> list:
    """è§£æ Wine-Searcher æœç´¢ç»“æœé¡µ"""
    results = []
    soup = BeautifulSoup(html, 'html.parser')

    # æ–¹æ³•1: ä¼˜å…ˆå°è¯•å¤šç§ CSS é€‰æ‹©å™¨ï¼ˆWine-Searcher é¡µé¢ç»“æ„å¯èƒ½å˜åŒ–ï¼‰
    selectors = [
        '.card__offer', '.offer-row', '.result-row', '[data-offer]',
        'tr.offer', '.search-result-item',
        '.wine-card', '.listing-row', '.price-listing',
        'div[class*="offer"]', 'div[class*="listing"]',
        'tr[class*="offer"]', 'tr[class*="result"]',
    ]
    offer_cards = []
    for sel in selectors:
        offer_cards = soup.select(sel)
        if offer_cards:
            logger.debug(f"HTML è§£æå‘½ä¸­é€‰æ‹©å™¨: {sel} ({len(offer_cards)} æ¡)")
            break

    for card in offer_cards[:20]:
        try:
            merchant_el = card.select_one('.merchant-name, .offer-merchant, a[data-merchant]')
            merchant = merchant_el.get_text(strip=True) if merchant_el else "æœªçŸ¥å•†å®¶"

            # ğŸ›‘ è¿‡æ»¤ 1: æ’é™¤æ‹å–å’Œæ•´ç®±
            card_text = card.get_text().lower()
            if any(kw in card_text for kw in ['auction', 'bid ', 'lot of', 'case of', 'set of']):
                continue
            if 'auction' in merchant.lower():
                continue

            price_el = card.select_one('.offer-price, .price, [data-price]')
            if not price_el:
                continue
            price_text = price_el.get_text(strip=True)
            price = _parse_price(price_text)
            
            # ğŸ›‘ è¿‡æ»¤ 2: æ’é™¤ä»·æ ¼è¿‡ä½ï¼ˆå¯èƒ½æ˜¯é…ä»¶æˆ–è¯¯æŠ¥ï¼‰
            if not price or price < 20:
                continue

            currency = _detect_currency(price_text)

            country_el = card.select_one('.country, .offer-country, [data-country]')
            country = country_el.get_text(strip=True) if country_el else ""

            # ä¼˜å…ˆè·å– wine-searcher.com çš„é“¾æ¥ï¼ˆå…·ä½“ listingï¼‰ï¼Œè€Œéé…’å•†ä¸»é¡µ
            link = ""
            for a_tag in card.select('a[href]'):
                href = a_tag.get('href', '')
                if 'wine-searcher.com' in href or href.startswith('/find') or href.startswith('/merchant'):
                    link = href
                    break
            if link and not link.startswith('http'):
                link = BASE_URL + link

            results.append({
                "merchant": merchant,
                "price": price,
                "price_usd": _to_usd(price, currency),
                "currency": currency,
                "country": country,
                "url": link,
            })
        except Exception as e:
            logger.debug(f"è§£æ offer å¤±è´¥: {e}")

    # æ–¹æ³•2: JSON-LD
    if not results:
        for script in soup.select('script[type="application/ld+json"]'):
            try:
                data = json.loads(script.string)
                if isinstance(data, dict) and 'offers' in data:
                    offers = data['offers']
                    if isinstance(offers, list):
                        for offer in offers[:20]:
                            # ğŸ›‘ JSON-LD è¿‡æ»¤
                            desc = (offer.get('description') or '').lower()
                            seller = (offer.get('seller', {}).get('name') or '').lower()
                            if any(kw in desc or kw in seller for kw in ['auction', 'bid', 'lot of']):
                                continue

                            price = offer.get('price')
                            if price and float(price) > 20:
                                curr = offer.get('priceCurrency', 'USD')
                                results.append({
                                    "merchant": offer.get('seller', {}).get('name', 'æœªçŸ¥'),
                                    "price": float(price),
                                    "price_usd": _to_usd(float(price), curr),
                                    "currency": curr,
                                    "country": "",
                                    "url": offer.get('url', ''),
                                })
            except Exception:
                continue

    # æ–¹æ³•3: ç®€å•ä»·æ ¼æå–ï¼ˆfallbackï¼‰â€” ä¸¥æ ¼é™åˆ¶èŒƒå›´
    if not results:
        # å°è¯•ä»é¡µé¢ä¸­ç›´æ¥æå–ä»·æ ¼æ•°å­—
        price_patterns = soup.find_all(string=re.compile(r'\$[\d,]+\.?\d*'))
        for pt in price_patterns[:5]:
            price = _parse_price(pt)
            # ä¸¥æ ¼é™åˆ¶ï¼šå•ç“¶è‘¡è„é…’ä»·æ ¼é€šå¸¸åœ¨ $20-$15000 ä¹‹é—´
            if price and 20 < price < 15000:
                results.append({
                    "merchant": "Wine-Searcher",
                    "price": price,
                    "price_usd": price,
                    "currency": "USD",
                    "country": "",
                    "url": "",
                })

    return results


# â”€â”€ å…¬å¼€ API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def search_wine_prices(wine_name: str, country_filter: str = None) -> list:
    """æœç´¢é…’ä»·"""
    search_query = wine_name.replace(' ', '+')
    url = f"{BASE_URL}/find/{search_query}/1/a"
    if country_filter:
        url += f"?Xcountry={country_filter}"

    await _random_delay(2, 5)
    html = await _smart_fetch(url)

    if not html:
        return []

    results = _parse_wine_page(html)

    # å¦‚æœæ˜¯é¦™æ¸¯é¡µé¢ï¼Œå¼ºåˆ¶å°†æœªæ ‡æ³¨ HKD çš„ $ ä»·æ ¼è§†ä¸º HKD
    if country_filter and 'hong' in country_filter.lower():
        for r in results:
            if r['currency'] == 'USD':
                # Wine-Searcher é¦™æ¸¯é¡µé¢é»˜è®¤æ˜¾ç¤º HKDï¼Œå³ä½¿ç¬¦å·æ˜¯ $
                r['currency'] = 'HKD'
                r['price_usd'] = _to_usd(r['price'], 'HKD')
                logger.debug(f"é¦™æ¸¯é¡µé¢è´§å¸ä¿®æ­£: ${r['price']:.0f} HKD -> ${r['price_usd']:.2f} USD")

    return results


async def get_global_lowest_price(wine_name: str) -> Optional[dict]:
    """è·å–å…¨çƒæœ€ä½ä»·"""
    results = await search_wine_prices(wine_name)
    if not results:
        return None
    results.sort(key=lambda x: x["price_usd"])
    return results[0]


async def get_hk_average_price(wine_name: str) -> Optional[float]:
    """è·å–é¦™æ¸¯å¸‚åœºå‡ä»·ï¼ˆå«å¼‚å¸¸å€¼è¿‡æ»¤ï¼‰"""
    results = await search_wine_prices(wine_name, country_filter="hong+kong")
    if not results:
        return None

    usd_prices = [r["price_usd"] for r in results if r["price_usd"] > 0]
    if not usd_prices:
        return None

    # å¼‚å¸¸å€¼è¿‡æ»¤ï¼šå»æ‰åç¦»ä¸­ä½æ•° 5 å€ä»¥ä¸Šçš„å€¼
    usd_prices.sort()
    median = usd_prices[len(usd_prices) // 2]
    filtered = [p for p in usd_prices if p < median * 5 and p > median * 0.2]
    if not filtered:
        filtered = usd_prices  # è¿‡æ»¤å¤±è´¥åˆ™å›é€€

    avg = sum(filtered) / len(filtered)
    logger.info(f"é¦™æ¸¯å‡ä»· ({wine_name}): ${avg:.2f} USD (æ ·æœ¬ {len(filtered)} æ¡, ä¸­ä½ ${median:.2f})")
    return avg


async def search_wine_basic(wine_name: str) -> dict:
    """æœç´¢ä¸€æ¬¾é…’åŸºæœ¬ä¿¡æ¯"""
    global_lowest = await get_global_lowest_price(wine_name)
    if not global_lowest:
        return {"wine_name": wine_name, "found": False}

    # æ„å»º Wine-Searcher æœç´¢ç»“æœé¡µ URL ä½œä¸ºç»Ÿä¸€ç›´è¾¾é“¾æ¥
    search_query = wine_name.replace(' ', '+')
    ws_search_url = f"{BASE_URL}/find/{search_query}/1/a"

    # å¦‚æœçˆ¬åˆ°çš„ url æ˜¯é…’å•†ä¸»é¡µï¼ˆé wine-searcherï¼‰ï¼Œæ›¿æ¢ä¸ºæœç´¢é¡µ
    if global_lowest.get("url") and 'wine-searcher.com' not in global_lowest["url"]:
        global_lowest["url"] = ws_search_url
    elif not global_lowest.get("url"):
        global_lowest["url"] = ws_search_url

    await _random_delay(3, 6)

    hk_avg = await get_hk_average_price(wine_name)

    return {
        "wine_name": wine_name,
        "found": True,
        "global_lowest": global_lowest,
        "hk_avg_price_usd": hk_avg,
    }
